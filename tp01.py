# -*- coding: utf-8 -*-
"""TP01.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ZMgenMgoY799LE8X9EelI82OPg6XoT8b

# TP1 - Speaker verification

## Etape 1 : Installation du toolkit Kiwano

Kiwano est une boîte à outils open source pour la vérification des locuteurs basée sur PyTorch. L'objectif de ce TP est d'implémenter chaîne basique de vérification du locuteur en passant par les étapes suivantes : extraction de filtres de banques de fréquences,  normalisation CMVN, extraction d’embeddings du locuteur et calcul de la similarité avec une mesure du cosinus.
"""

# ! git clone https://github.com/mrouvier/kiwano/
# ! cd kiwano/ && pwd
# ! cd kiwano/ && pip install -r requirements.txt
# ! cd kiwano/ && pip install -e .

"""## Etape 2 : Extraction des filter banks

### Objectif
L'objectif est d'analyser la manière dont les caractéristiques acoustiques d'un signal audio sont représentées à travers des coefficients issus des filter banks. Ce code permet de charger un fichier audio (ici enrollment.wav) et d'en extraire les filter banks.

### Questions
- A partir des filter banks, pouvez vous donnez la durée du fichier audio ?
- Quel est le nombre de coefficient extrait ?
"""

import torchaudio
from IPython.display import Audio
import kiwano
from kiwano.features import Fbank

#Initialisation de l'extracteur de features (Filter Bank)
feature_extractor = Fbank()

#Chargement d'un fichier audio
wav, sr = torchaudio.load("enrollment.wav")

#Lecture et affichage du fichier audio directement dans le notebook
display(Audio("enrollment.wav", autoplay=True))

#Extraction des features
f = feature_extractor.extract(wav, sampling_rate=sr)


#Affichage
print(f)

"""- wav = nbr d'echantillons
- sr = frequance
"""

wav.shape[1]

num_samples = wav.shape[1]
duration = num_samples / sr
print(f"Durée du fichier audio : {duration:} s")

"""- f.shape[1] deuxieme dimension de la matrice ==> nbr coef pour chaque frame"""

coef=f.shape[1]
print(f"nombre de coef : {coef}")

"""## Etape 3 : Normalisation CMVN

### Questions
- Que fait la normalisation CMVN dans le code ?
- Pourquoi est-il nécessaire de réaliser cette normalisation ?






"""

import torch
import torchaudio
import kiwano
from kiwano.augmentation import CMVN

"""### Réponse 03:
- 1-Le but du CMVN est de normaliser les coefficients des caractéristiques acoustiques elle fait : Normalisation de la moyenne (recentre les coefficients autour de zéro)  et Normalisation de la variance (met les coefficients à la même échelle)
- 2-garantit une échelle cohérente pour toutes les caractéristiques ce qui facilite l'apprentissage pour les modèles.Et pour permettre d'obtenir des caractéristiques comparables entre différentes parties du signal audio.
"""

#Initialisation de la normalisation CMVN
cmvn = CMVN()

#Initialisation de l'extracteur de features (Filter Bank)
feature_extractor = Fbank()

#Chargement d'un fichier audio
wav, sr = torchaudio.load("enrollment.wav")

#Extraction des features
f = feature_extractor.extract(wav, sampling_rate=sr)

#Affichage de la matrice
print(f)

#Application de la normalisation CMVN
f_cmvn = cmvn(f)

#Affichage de la matrice
print(f_cmvn)

m = torch.mean(f, dim=0)

print(f-m)

f_cmvn.shape[0]

"""## Etape 4 : Extraction d'un speaker embedding

### Objectif
L'objectif de cette étape est d'extraire les embeddings du locuteur à l'aide d'un modèle pré-entraîné. Le code actuel permet d'extraire un embedding à partir du fichier audio "enrollment.wav".

### Exercice
- Extraire un embedding à partir du fichier "test1.wav".
- De quelle taille est l'embedding ?
- Comparer les embeddings issue du fichier "enrollment.wav" et "test1.wav". La comparaison se fait en calculant un score de similarité à l'aide de la distance cosinus (cosine similarity).

- Extraire un embedding à partir du fichier "enrollment.wav".
- Comparer les embeddings issue du fichier "enrollment.wav" et "test2.wav". La comparaison se fait en calculant un score de similarité à l'aide de la distance cosinus (cosine similarity).

- Ecouter les fichiers audio "enrollment.wav", "test1.wav" et "test2.wav". En fonction de ce que vous entendez, quel locuteur semble le plus proche de "enrollment.wav" ? Que révèlent les scores de similarité calculés à propos des ressemblances entre les locuteurs ?


"""

import torch
import kiwano
from kiwano.features import Fbank
from kiwano.augmentation import CMVN
from kiwano.model import ResNetV2

wav_test1, sr_test1 = torchaudio.load("test1.wav")
wav_test2, sr_test2 = torchaudio.load("test2.wav")

#Lecture et affichage du fichier audio directement dans le notebook
display(Audio("enrollment.wav", autoplay=True))
display(Audio("test1.wav", autoplay=True))
display(Audio("test2.wav", autoplay=True))

#Initialisation de la normalisation CMVN et de l'extracteur de features
cmvn = CMVN()
feature_extractor = Fbank()

#Creation du modèle ResNet et initialisation des poids
resnet_model = ResNetV2(num_classes=21000)
resnet_model.load_state_dict(torch.load("model/model.ckpt", map_location="cpu")["model"])
resnet_model.eval()

#Chargement d'un fichier audio
wav, sr = torchaudio.load("enrollment.wav")

##Extraction des features et application de la normalisation CMVN
f = cmvn(
    feature_extractor.extract(wav, sampling_rate=sr)
)

#Extraction de l'embedding
f = f.unsqueeze(0).unsqueeze(1)
emb = resnet_model(f)

#print(emb[0])

#Question 01:
f_test1=cmvn(feature_extractor.extract(wav_test1,sampling_rate=sr))
f_test1=f_test1.unsqueeze(0).unsqueeze(1)
emb_test1=resnet_model(f_test1)
print(emb_test1[0])

#Question 02:
print(f"la taille de l'embedding test 01 est {emb_test1[0].shape}")

from torch.nn.functional import cosine_similarity

#Question 03:
similarity_enroll_test1 = cosine_similarity(emb[0], emb_test1[0], dim=0)
print(f"Similarité entre enrollment et test1 :{similarity_enroll_test1}")

#Question04
f_test2=cmvn(feature_extractor.extract(wav_test2,sampling_rate=sr))
f_test2=f_test2.unsqueeze(0).unsqueeze(1)
emb_test2=resnet_model(f_test2)
print(emb_test2[0])

similarity_enroll_test2 = cosine_similarity(emb[0], emb_test2[0], dim=0)
print(f"Similarité entre enrollment et test2 :{similarity_enroll_test2}")

similarity_test1_test2=cosine_similarity(emb_test2[0],emb_test1[0],dim=0)
print(f"Similarité entre test2 et test1: {similarity_test1_test2}")

"""## Etape 5 : Bruit blanc

Le bruit blanc est un signal sonore qui comprend toutes les fréquences audibles (ou toutes les valeurs possibles dans un domaine donné), avec une intensité ou puissance uniforme pour chaque fréquence. Il se manifeste par un son constant, similaire à celui d'une télévision ou d'une radio mal réglée. Le terme "blanc" est employé par analogie avec la lumière blanche, qui contient toutes les longueurs d'onde du spectre visible. Dans cet exercice, nous allons appliquer du bruit blanc à chaque fichier audio et observer l'impact que cela a sur le modèle.

### Exercice
- Répondez aux questions de l'étape 4, mais cette fois en ajoutant du bruit blanc à chaque fichier audio.
- Pourquoi la distance entre les fichiers diffère-t-elle ? Quelle conclusion peut-on en tirer ?






"""

import kiwano
from kiwano.features import Fbank
from kiwano.augmentation import CMVN
from kiwano.model import ResNetV2
import IPython.display as ipd

def white_noise(waveform):
    noise_level = 0.01
    white_noise = torch.randn(waveform.shape) * noise_level
    waveform_with_noise = waveform + white_noise
    waveform_with_noise = torch.clamp(waveform_with_noise, -1.0, 1.0)
    return waveform_with_noise


cmvn = CMVN()
feature_extractor = Fbank()
wav, sr = torchaudio.load("enrollment.wav")
#Question 01:
wav_test1, sr_test1 = torchaudio.load("test1.wav")
wav_test2, sr_test2 = torchaudio.load("test2.wav")
wav_with_noise = white_noise(wav)
wav_test1_noise = white_noise(wav_test1)
wav_test2_noise = white_noise(wav_test2)

display(ipd.Audio(wav, rate=sr, autoplay=True))
display(ipd.Audio(wav_with_noise, rate=sr, autoplay=True))

resnet_model = ResNetV2(num_classes=21000)
resnet_model.load_state_dict(torch.load("model/model.ckpt", map_location="cpu")["model"])
resnet_model.eval()

f_enroll_noise = cmvn(
    feature_extractor.extract(wav_with_noise, sampling_rate=sr)
)
f_test1_noise = cmvn(feature_extractor.extract(wav_test1_noise, sampling_rate=sr_test1))
f_test2_noise = cmvn(feature_extractor.extract(wav_test2_noise, sampling_rate=sr_test2))


f_enroll_noise = f_enroll_noise.unsqueeze(0).unsqueeze(1)
f_test1_noise = f_test1_noise.unsqueeze(0).unsqueeze(1)
f_test2_noise = f_test2_noise.unsqueeze(0).unsqueeze(1)


emb_enroll_noise = resnet_model(f_enroll_noise)
emb_test1_noise = resnet_model(f_test1_noise)
emb_test2_noise = resnet_model(f_test2_noise)

#print(emb[0])

print(f"la taille de l'embedding est :{emb_enroll_noise[0].shape}")
print(f"la taille de l'embedding est :{emb_test1_noise[0].shape}")
print(f"la taille de l'embedding est :{emb_test2_noise[0].shape}")

display(ipd.Audio(wav_test1_noise,rate=sr_test1,autoplay=True))

display(ipd.Audio(wav_test2_noise,rate=sr_test2,autoplay=True))

similarity_enroll_test1_noise = cosine_similarity(emb_enroll_noise[0], emb_test1_noise[0], dim=0)
print(f"Similarité entre enrollment et test1 :{similarity_enroll_test1_noise}")

similarity_enroll_test2_noise = cosine_similarity(emb_enroll_noise[0], emb_test2_noise[0], dim=0)
print(f"Similarité entre enrollment et test2 :{similarity_enroll_test2_noise}")

"""- Question 02:

Le bruit peut réduire la précision de l'embedding extrait, car les caractéristiques importantes de la voix peuvent être noyées dans le bruit.

## Etape 6 : Calculer l'EER

### Objectif
L'objectif de cette étape est de calculer l'EER (Equal Error Rate) en utilisant un fichier de "trials". L'EER est une mesure utilisée pour évaluer les performances d'un système de reconnaissance vocale ou d'authentification de locuteur. Il représente le point où le taux de fausses acceptations (False Acceptance Rate, FAR) est égal au taux de faux rejets (False Rejection Rate, FRR).

Dans le fichier "trials.txt", chaque ligne contient trois colonnes :
- Première colonne : fichier d'enrollment (enregistrement de référence du locuteur).
- Deuxième colonne : fichier de test (enregistrement à comparer).
- Troisième colonne : indication si les deux fichiers appartiennent au même locuteur (target, noté par 1) ou à des locuteurs différents (non-target, noté par 0).


### Exercice
- Calculer l'EER du modèle en utilisant les paires d'enregistrement dans le fichier "trials.txt".
- Dessinez la courbe DET du système sur ce corpus.

### Fichier trials.txt

Quelques lignes du fichier trials.txt
"""

! head trials.txt

"""### Calculer EER

Exemple pour calculer un EER. L'EER prend en entrée labels et scores.
"""

import sklearn.metrics
import numpy as np

labels = [0, 0, 0, 1, 1, 1, 1]
scores = [0.1, 0.2, 0.3, 0.4, 0.5, 0.1, 0.6]

fpr, tpr, threshold = sklearn.metrics.roc_curve(labels, scores, pos_label=1)
fnr = 1 - tpr

eer_1 = fpr[np.nanargmin(np.absolute((fnr - fpr)))]
eer_2 = fnr[np.nanargmin(np.absolute((fnr - fpr)))]
eer = (eer_1 + eer_2) / 2

print(eer*100)

"""L'EER est le point où le FAR (False Acceptance Rate) et le FRR (False Rejection Rate) sont égaux ou très proches.

 L'EER est une mesure unique qui donne une idée générale des performances du système.

### REP partie 06
"""

from tqdm import tqdm
import os

trials = []
# with open("trials.txt", "r") as f:
#     for line in f:
#         trials.append(line.strip().split())

with open("trials.txt", "r") as f:
    for i, line in enumerate(f):
        if i >= 10:
            break
        trials.append(line.strip().split())

enrollment_files = [trial[0] for trial in trials]
test_files = [trial[1] for trial in trials]
labels = [int(trial[2]) for trial in trials]

cmvn = CMVN()
feature_extractor = Fbank()

# modèle ResNet
resnet_model = ResNetV2(num_classes=21000)
resnet_model.load_state_dict(torch.load("model/model.ckpt", map_location="cpu")["model"], strict=False)
resnet_model.eval()

scores = []

for enroll_file, test_file in tqdm(zip(enrollment_files, test_files), total=len(enrollment_files), desc="Calcul des similarités"):
    # Chargement des fichiers audio
    wav_enroll, sr_enroll = torchaudio.load(enroll_file)
    wav_test, sr_test = torchaudio.load(test_file)
    #Embeddings
    f_enroll = cmvn(feature_extractor.extract(wav_enroll, sampling_rate=sr_enroll))
    f_test = cmvn(feature_extractor.extract(wav_test, sampling_rate=sr_test))

    f_enroll = f_enroll.unsqueeze(0).unsqueeze(1)
    f_test = f_test.unsqueeze(0).unsqueeze(1)

    emb_enroll = resnet_model(f_enroll)
    emb_test = resnet_model(f_test)

    # similarité cosinus
    similarity = cosine_similarity(emb_enroll[0], emb_test[0], dim=0).item()
    scores.append(similarity)

fpr, tpr, threshold = sklearn.metrics.roc_curve(labels, scores, pos_label=1) #True Positive Rate
fnr = 1 - tpr

# Calcul de l'EER
eer_1 = fpr[np.nanargmin(np.absolute((fnr - fpr)))] #False Positive Rate
eer_2 = fnr[np.nanargmin(np.absolute((fnr - fpr)))] #False Negative Rate
eer = (eer_1 + eer_2) / 2

print(f"EER : {eer * 100:.2f}%")

"""## Etape 7 : Normaliser les speaker embeddings

## Objectif
L'objectif de cette étape est de normaliser les *speaker embeddings* en les centrant par rapport à une moyenne calculée à partir des fichiers audio présents dans le répertoire train/. Cette normalisation permet de réduire les informations non pertinentes et d'obtenir une meilleure représentation des locuteurs.

### Exercice
- **Calcul des moyennes des speaker embeddings** : Calculez la moyenne des *speaker embeddings* issus des fichiers audio du répertoire train/. Cette moyenne servira de référence pour centrer les représentations des locuteurs.
- **Normalisation des embeddings** : Normalisez les *speaker embeddings* des enregistrements présents dans le fichier *trials.txt* en soustrayant cette moyenne calculée.
- **Recalculez l'EER en utilisant les speaker embeddings normalisés** : Recalculez l'EER en utilisant les *speaker embeddings* normalisés sur les paires d'enregistrement présentes dans le fichier *trials.txt*. Quels changements observez-vous après la normalisation ?


"""

import os

cmvn = CMVN()
feature_extractor = Fbank()

# modèle ResNet
resnet_model = ResNetV2(num_classes=21000)
resnet_model.load_state_dict(torch.load("model/model.ckpt", map_location="cpu")["model"], strict=False)
resnet_model.eval()

trials=[]
with open("trials.txt", "r") as f:
    for i, line in enumerate(f):
        if i >= 10:
            break
        trials.append(line.strip().split())

enrollment_files = [trial[0] for trial in trials]
test_files = [trial[1] for trial in trials]
labels = [int(trial[2]) for trial in trials]

embeddings = []

for i, filename in enumerate(os.listdir("train/")):
    if i >= 10:
        break
    if filename.endswith('.wav'):  # fichier audio ?
        wav, sr = torchaudio.load(os.path.join("train/", filename))

        features = cmvn(feature_extractor.extract(wav, sampling_rate=sr))
        features = features.unsqueeze(0).unsqueeze(1)

        embedding = resnet_model(features)
        embeddings.append(embedding[0])

# la moyenne
moy_embedding = torch.mean(torch.stack(embeddings), dim=0)
print(f"Moyenne des embeddings calculée : {moy_embedding.shape}")

normalized_scores = []

# Parcourir les paires d'enroll et de test dans le fichier trials.txt
for enroll_file, test_file in tqdm(zip(enrollment_files, test_files), total=len(enrollment_files)):
    wav_enroll, sr_enroll = torchaudio.load(enroll_file)
    wav_test, sr_test = torchaudio.load(test_file)

    # les embeddings
    f_enroll = cmvn(feature_extractor.extract(wav_enroll, sampling_rate=sr_enroll))
    f_test = cmvn(feature_extractor.extract(wav_test, sampling_rate=sr_test))

    f_enroll = f_enroll.unsqueeze(0).unsqueeze(1)
    f_test = f_test.unsqueeze(0).unsqueeze(1)

    emb_enroll = resnet_model(f_enroll)[0]
    emb_test = resnet_model(f_test)[0]

    # normaliser les embeddings en soustrayant la moyenne
    emb_enroll_normalized = emb_enroll - moy_embedding
    emb_test_normalized = emb_test - moy_embedding

    # similarité cosinus avec les embeddings normalisés
    similarity = cosine_similarity(emb_enroll_normalized, emb_test_normalized, dim=0).item()
    normalized_scores.append(similarity)

fpr, tpr, threshold = sklearn.metrics.roc_curve(labels, normalized_scores, pos_label=1)
fnr = 1 - tpr

eer_1 = fpr[np.nanargmin(np.absolute((fnr - fpr)))]
eer_2 = fnr[np.nanargmin(np.absolute((fnr - fpr)))]
eer = (eer_1 + eer_2) / 2

print(f"EER après normalisation : {eer * 100:.2f}%")

"""## Etape 8 : ResNet

### Question
- En regardant le code source de Kiwano, donnez l'architecture du ResNet ?

Couche de prétraitement

Embeddings

Fonction de perte

## Etape 9 : Voice Activity Detection (exercice optionnel)

### Objectif
Pour ceux qui ont terminé, certains fichiers audio peuvent contenir des zones de silence. Ces silences peuvent affecter la qualité des embeddings. L'objectif de cet exercice est de supprimer ces zones de silence dans chaque fichier audio en utilisant l'outil SILERO : https://github.com/snakers4/silero-vad

### Exercice
- Ouvrir un fichier audio.
- Détecter les zones de silence à l'aide de Silero VAD.
- Supprimer ces zones de silence.
- Calculer l'embedding à partir du fichier audio nettoyé.
"""

from silero_vad import load_silero_vad, read_audio, get_speech_timestamps

model = load_silero_vad()
wav = read_audio("enrollment.wav")
speech_timestamps = get_speech_timestamps(wav, model)

wav_witout_silence = collect_chunks(speech_timestamps, wav)